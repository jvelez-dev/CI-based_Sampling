{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchview\n",
      "  Downloading torchview-0.2.6-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: torchview\n",
      "Successfully installed torchview-0.2.6\n"
     ]
    }
   ],
   "source": [
    "#!pip install gym\n",
    "#!pip install torch\n",
    "#!pip install causalinference\n",
    "#!pip install moviepy\n",
    "#!pip install pygame\n",
    "#!pip install gym[classic_control]\n",
    "#!pip install torchviz\n",
    "#!pip install torchview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from causalinference import CausalModel\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api  as smf\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import islice\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy import stats\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython import display\n",
    "from IPython.display import Image\n",
    "\n",
    "from torchviz import make_dot\n",
    "from torchview import draw_graph\n",
    "\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cm = Causal_Model()\n",
    "#cm.unificar()\n",
    "#ps1 = cm.calc_propensity_score(1,\"PropScore\") #T = Derecha\n",
    "#ps2 = cm.calc_propensity_score(0,\"PropScore\") #T =Izquierda\n",
    "#pares = cm.KNN_matching(ps1['PropScore'].to_numpy(),ps2['PropScore'].to_numpy())\n",
    "#has_significant_difference,difference_magnitude = cm.calc_dif_duracion_promedio(pares,ps1,ps2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Causal_Model():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def crear_data_frame(self,memory):\n",
    "        data=[]\n",
    "        for i in memory:\n",
    "           #print(i)\n",
    "            if i[0][2] is not None:\n",
    "                i2 = i[0][2].numpy()\n",
    "                i3 = i[0][3].numpy()\n",
    "            else:\n",
    "                i2=np.array([[0,0,0,0]])\n",
    "                i3=np.array([0])#<-- Las rew no siempre son 1\n",
    "            lst1 = i[0][0].numpy().reshape(-1).tolist()\n",
    "            lst2 = i[0][1].numpy().reshape(-1).tolist()\n",
    "            lst3 = i2.reshape(-1).tolist()\n",
    "            lst4 = i3.reshape(-1).tolist()\n",
    "            data=data+lst1+lst2+lst3+lst4+[i[1]]\n",
    "        arr = np.array(data)\n",
    "        arr = np.reshape(arr,(-1,11))\n",
    "        df = pd.DataFrame(arr,columns=['E1','E2','E3','E4','A','EF1','EF2','EF3','EF4','R','D'])\n",
    "        return df\n",
    "        \n",
    "    def calc_propensity_score(self, df, accion, ps=\"\"):\n",
    "        #df = pd.read_csv(\"memory2.csv\")\n",
    "        df.columns=['E1','E2','E3','E4','A','EF1','EF2','EF3','EF4','R','D']\n",
    "        #Filtar Acción\n",
    "        ps1 = pd.DataFrame(df[df.A==accion])\n",
    "\n",
    "        #Dividir en caracteristicas y target\n",
    "        caracteristicas=['E1','E2','E3','E4','A','EF1','EF2','EF3','EF4','D']  \n",
    "        X=ps1[caracteristicas]\n",
    "        Y=ps1.R\n",
    "\n",
    "        logreg = LogisticRegression(max_iter=2000,random_state=32)\n",
    "\n",
    "        # ajustar el modelo a los datos\n",
    "        logreg.fit(X, Y)\n",
    "\n",
    "        y_pred = logreg.predict_proba(X)\n",
    "\n",
    "        ps1[ps]=y_pred[:,1]\n",
    "        ps1 = ps1.sort_values(ps)\n",
    "        return ps1\n",
    "\n",
    "    def KNN_matching(self,propensity_scores_1,propensity_scores_2):\n",
    "        # Número de vecinos a considerar\n",
    "        k_neighbors = 1\n",
    "\n",
    "        # Crear el objeto NearestNeighbors para ambos conjuntos\n",
    "        nn_1 = NearestNeighbors(n_neighbors=k_neighbors)\n",
    "        nn_2 = NearestNeighbors(n_neighbors=k_neighbors)\n",
    "\n",
    "        # Ajustar el modelo de vecinos más cercanos para ambos conjuntos\n",
    "        nn_1.fit(propensity_scores_1.reshape(-1, 1))\n",
    "        nn_2.fit(propensity_scores_2.reshape(-1, 1))\n",
    "\n",
    "        # Encontrar los índices de los vecinos más cercanos para cada observación en ambos conjuntos\n",
    "        neighbors_indices_1 = nn_1.kneighbors(propensity_scores_2.reshape(-1, 1), return_distance=False)\n",
    "        neighbors_indices_2 = nn_2.kneighbors(propensity_scores_1.reshape(-1, 1), return_distance=False)\n",
    "\n",
    "        # Ahora `neighbors_indices_1` contiene los índices de los vecinos más cercanos en el conjunto 1\n",
    "        # para cada observación en el conjunto 2, y viceversa para `neighbors_indices_2`.\n",
    "\n",
    "        matched_pairs = []\n",
    "        # Emparejar las observaciones\n",
    "        for i, neighbor_index in enumerate(neighbors_indices_1):\n",
    "            # Emparejar la observación `i` del conjunto 2 con su vecino más cercano en el conjunto 1\n",
    "            matched_pairs.append((i, neighbor_index[0]))\n",
    "        # matched_pairs contendrá los índices de las observaciones emparejadas en ambos conjuntos\n",
    "        #print(matched_pairs)\n",
    "        return matched_pairs        \n",
    "    \n",
    "    def calc_dif_duracion_promedio(self,pares,ps1,ps2): #<-- Voy acá\n",
    "        df = pd.DataFrame(pares,columns=['C2','C1'])\n",
    "        UC1=[]\n",
    "        UC2=[]\n",
    "        for p in pares:                \n",
    "            UC2.append(ps2.iloc[p[0]]['D'])\n",
    "            UC1.append(ps1.iloc[p[1]]['D'])\n",
    "        df['UC2']=UC2\n",
    "        df['UC1']=UC1        \n",
    "        df['Diff']=df['UC2']-df['UC1']\n",
    "        #print(\"La estimación del efecto causal es {}\".format(df['Diff'].mean()))\n",
    "        #print(\"La DS de las diferencias es {}\".format(df['Diff'].std()))\n",
    "        diff = df[\"Diff\"].values.tolist()\n",
    "        t_statistic, p_value = stats.ttest_rel(df['UC2'],df['UC1'])\n",
    "        # Imprimir los resultados\n",
    "        #print(\"T-Statistic:\", t_statistic)\n",
    "        #print(\"p-Value:\", p_value)\n",
    "\n",
    "        # Comparar con el nivel de significancia\n",
    "        alpha = 0.05\n",
    "        if p_value < alpha:\n",
    "            #print(\"Diferencia significativa entre los tratamientos.\")\n",
    "            difference_magnitude = abs(np.mean(df['UC2']) - np.mean(df['UC1']))\n",
    "            has_significant_difference = True\n",
    "        else:\n",
    "            #print(\"No hay evidencia de diferencia significativa.\") \n",
    "            difference_magnitude = 0\n",
    "            has_significant_difference = False\n",
    "        return (has_significant_difference,difference_magnitude)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward')) \n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        self.priorities = []  # Lista para almacenar las prioridades\n",
    "        self.min_episodios = 500\n",
    "        self.cant_episodios = 0\n",
    "        self.cm = Causal_Model()\n",
    "    \n",
    "    def push(self, state, action, next_state, reward, episode_dur):  \n",
    "        if self.cant_episodios <= self.min_episodios:\n",
    "            self.memory.append((self.Transition(state,action,next_state,reward),episode_dur))\n",
    "            self.priorities.append(0)\n",
    "            # Ajustar las prioridades si es necesario\n",
    "            self.adjust_priorities(False,0)\n",
    "            self.cant_episodios +=1\n",
    "        else: #self.cant_episodios > self.min_episodios:\n",
    "            # Realizar el proceso de inferencia causal aquí\n",
    "            df = self.cm.crear_data_frame(self.memory)\n",
    "            df.columns=['E1','E2','E3','E4','A','EF1','EF2','EF3','EF4','R','D']\n",
    "            ps1 = self.cm.calc_propensity_score(df,1,\"PropScore\") #T = Derecha\n",
    "            ps2 = self.cm.calc_propensity_score(df,0,\"PropScore\") #T =Izquierda\n",
    "            pares = self.cm.KNN_matching(ps1['PropScore'].to_numpy(),ps2['PropScore'].to_numpy())\n",
    "            has_significant_difference,difference_magnitude=self.cm.calc_dif_duracion_promedio(pares,ps1,ps2)\n",
    "            \n",
    "            # Agregar la transición y sus resultados de inferencia causal al buffer y las prioridades\n",
    "            self.memory.append((self.Transition(state,action,next_state,reward),episode_dur))                   \n",
    "            self.priorities.append(difference_magnitude if has_significant_difference else 0)\n",
    "            # Ajustar las prioridades si es necesario\n",
    "            self.adjust_priorities(has_significant_difference,difference_magnitude)\n",
    "        \n",
    "    def adjust_priorities(self, has_significant_difference, difference_magnitude):\n",
    "        # Ajustar las prioridades en función de los resultados de la inferencia causal\n",
    "        # Puedes usar difference_magnitude o cualquier criterio que decidas\n",
    "        self.priorities = [item if has_significant_difference else 0 for item, has_significant_difference in zip(self.priorities, self.memory)]\n",
    "    \n",
    "    def sample(self,batch_size):\n",
    "        priorities = np.array(self.priorities)\n",
    "        valid_indices = np.where(np.isfinite(priorities))[0]  # Encuentra índices válidos (sin NaN)\n",
    "        \n",
    "        probabilities = []\n",
    "        if len(valid_indices) > 0:\n",
    "            valid_priorities = priorities[valid_indices]\n",
    "            max_priority = np.max(valid_priorities)\n",
    "            scaled_priorities = valid_priorities / (max_priority + 1e-6)  # Añadimos 1e-6 para evitar divisiones por cero\n",
    "\n",
    "            # Crear un array de probabilidades completo con valores escalados y 0 para los demás\n",
    "            full_scaled_probabilities = np.zeros(len(self.priorities))\n",
    "            full_scaled_probabilities[valid_indices] = scaled_priorities\n",
    "\n",
    "            probabilities_sum = np.sum(full_scaled_probabilities)\n",
    "\n",
    "            if probabilities_sum > 0:  # Evitamos divisiones por cero\n",
    "                probabilities = full_scaled_probabilities / probabilities_sum\n",
    "            else:\n",
    "                probabilities = np.ones(len(self.priorities)) / len(self.priorities)  # Distribución uniforme\n",
    "\n",
    "            indices_muestreo = np.random.choice(len(self.memory), batch_size, p=probabilities)\n",
    "            transiciones_muestreadas = [self.memory[i][0] for i in indices_muestreo]\n",
    "        else:\n",
    "            # Si no hay prioridades válidas, realiza un muestreo aleatorio\n",
    "            indices_muestreo = np.random.choice(len(self.memory), batch_size)\n",
    "            transiciones_muestreadas = [self.memory[i][0] for i in indices_muestreo]\n",
    "\n",
    "        return (transiciones_muestreadas,probabilities)  # Devuelve las transiciones muestreadas\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def save(self,duraciones):        \n",
    "        data=[]\n",
    "        for i in self.memory:\n",
    "            if i[2] is not None:\n",
    "                i2 = i[2].numpy()\n",
    "                i3 = i[3].numpy()\n",
    "            else:\n",
    "                i2=np.array([[0,0,0,0]])\n",
    "                i3=np.array([0])#<-- Las rew no siempre son 1\n",
    "            lst1 = i[0].numpy().reshape(-1).tolist()\n",
    "            lst2 = i[1].numpy().reshape(-1).tolist()\n",
    "            lst3 = i2.reshape(-1).tolist()\n",
    "            lst4 = i3.reshape(-1).tolist()\n",
    "            data=data+lst1+lst2+lst3+lst4\n",
    "        arr = np.array(data)\n",
    "        arr = np.reshape(arr,(-1,10))\n",
    "        df = pd.DataFrame(arr,columns=['E1','E2','E3','E4','A','EF1','EF2','EF3','EF4','R'])\n",
    "        df.to_csv(\"memory.csv\",index=False,header=None)\n",
    "        #arr_dur = np.array(duraciones)\n",
    "        #df_dur = pd.DataFrame(arr_dur)\n",
    "        #df_dur.to_csv(\"episode_dur.csv\",index=False,header=None)\n",
    "        \n",
    "        \n",
    "    def load(self):\n",
    "        df = pd.read_csv(\"memory.csv\")\n",
    "        return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "   \n",
    "    l2_lambda=0.01\n",
    "    \n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 64)\n",
    "        #self.dropout1 = nn.Dropout(p=0.3)\n",
    "        self.layer2 = nn.Linear(64, 64)\n",
    "        self.layer3 = nn.Linear(64, n_actions)\n",
    "\n",
    "    #def l2_regularization_loss(self):\n",
    "    #    l2_loss = 0\n",
    "    #    for param in self.parameters():\n",
    "    #        l2_loss += torch.norm(param, p=2)  # Calcula la norma L2 de los parámetros\n",
    "    #    return self.l2_lambda * l2_loss\n",
    "    \n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        #x = self.dropout1(x)\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "class Agente():\n",
    "    # BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "    # GAMMA is the discount factor as mentioned in the previous section\n",
    "    # EPS_START is the starting value of epsilon\n",
    "    # EPS_END is the final value of epsilon\n",
    "    # EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "    # TAU is the update rate of the target network\n",
    "    # LR is the learning rate of the ``AdamW`` optimizer\n",
    "    BATCH_SIZE = 64\n",
    "    GAMMA = 0.99\n",
    "    EPS_START = 0.9\n",
    "    EPS_END = 0.05\n",
    "    EPS_DECAY = 1000\n",
    "    TAU = 0.005\n",
    "    LR = 1e-4\n",
    "    Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward'))    \n",
    "    loss_history = []\n",
    "    num_episodes = 0\n",
    "\n",
    "\n",
    "    def __init__(self,device,env):\n",
    "        self.device = device\n",
    "        self.env = env    \n",
    "        # Get number of actions from gym action space\n",
    "        n_actions = env.action_space.n\n",
    "        # Get the number of state observations\n",
    "        state, info = env.reset()\n",
    "        n_observations = len(state)\n",
    "\n",
    "        self.policy_net = DQN(n_observations, n_actions).to(device)\n",
    "        self.target_net = DQN(n_observations, n_actions).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=self.LR, amsgrad=True)\n",
    "        self.memory = ReplayMemory(10000)\n",
    "\n",
    "        self.steps_done = 0\n",
    "        self.episode_durations = []\n",
    "        self.video_path = \"./recorder.mp4\"\n",
    "        self.video_recorder = VideoRecorder(env, self.video_path, enabled=self.video_path is not None)\n",
    "        \n",
    "        self.metricas_eval =[]\n",
    "        self.loss = 0.0\n",
    "        self.entropy_pol = 0.0\n",
    "        self.paso_episodio = 0.0\n",
    "        self.exploracion = 0.0\n",
    "        self.explotacion = 0.0\n",
    "        \n",
    "    \n",
    "    def save(self,file=''):\n",
    "         torch.save({'model_state_dict': self.policy_net.state_dict(),\n",
    "                     'optimizer_state_dict' : self.optimizer.state_dict(),}, \n",
    "                    file)\n",
    "\n",
    "    def load(self,file='',modo='train'):        \n",
    "        if os.path.isfile(file):            \n",
    "            print(\"=> loading checkpoint... \")\n",
    "            self.policy_net = DQN(n_observations, n_actions).to(device)\n",
    "            self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=self.LR, amsgrad=True)            \n",
    "            checkpoint = torch.load(file)\n",
    "            self.policy_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])  \n",
    "            if modo != 'train':\n",
    "                self.policy_net.eval()\n",
    "            else:\n",
    "                self.policy_net.train()\n",
    "            print(\"done !\")\n",
    "        else:\n",
    "            print(\"no checkpoint found...\")\n",
    "\n",
    "    def select_action(self, state):\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.EPS_END + (self.EPS_START - self.EPS_END) * \\\n",
    "            math.exp(-1. * self.steps_done / self.EPS_DECAY)\n",
    "        self.steps_done += 1\n",
    "        self.paso_episodio +=1\n",
    "        if sample > eps_threshold:\n",
    "            self.explotacion+=1\n",
    "            with torch.no_grad():\n",
    "                # t.max(1) will return the largest column value of each row.\n",
    "                # second column on max result is index of where max element was\n",
    "                # found, so we pick action with the larger expected reward.\n",
    "                return self.policy_net(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            self.exploracion+=1\n",
    "            return torch.tensor([[self.env.action_space.sample()]], device=self.device, dtype=torch.long)    \n",
    "\n",
    "\n",
    "    def optimize_model(self):        \n",
    "        if len(self.memory) < self.BATCH_SIZE:\n",
    "            return\n",
    "        transitions,probabilities = self.memory.sample(self.BATCH_SIZE)\n",
    "        \n",
    "        #measuring entropy of policy_net\n",
    "        self.entropy_pol = -np.sum([p * np.log(p) for p in probabilities if p > 0])\n",
    "                       \n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = self.Transition(*zip(*transitions))\n",
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                              batch.next_state)), device=self.device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                    if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        next_state_values = torch.zeros(self.BATCH_SIZE, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0]\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * self.GAMMA) + reward_batch\n",
    "\n",
    "        # Compute Huber loss\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        #l2_loss = self.policy_net.l2_regularization_loss()\n",
    "        #loss += l2_loss\n",
    "        \n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # In-place gradient clipping\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "        # Registrar el valor de la pérdida en el historial\n",
    "        self.loss_history.append(loss.item())\n",
    "        self.loss += loss.item()        \n",
    "        \n",
    "    def graficar_aprendizaje(self):\n",
    "        # Graficar el historial de pérdidas\n",
    "        plt.plot(self.loss_history)        \n",
    "        plt.xlabel('Episodio')\n",
    "        plt.ylabel('Pérdida')       \n",
    "        plt.ylim(0,1)\n",
    "        plt.title('Progreso de la Pérdida durante el Entrenamiento')        \n",
    "        plt.show()   \n",
    "\n",
    "    def plot_durations1(self):\n",
    "        plt.figure(1)\n",
    "        durations_t = torch.tensor(self.episode_durations, dtype=torch.float)\n",
    "        plt.title('Result')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Duration')\n",
    "        plt.plot(durations_t.numpy())\n",
    "        means = durations_t.unfold(0, self.num_episodes, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(self.num_episodes-1), means))\n",
    "        plt.plot(means.numpy())\n",
    "        \n",
    "    def plot_durations(self):\n",
    "        plt.figure(1)\n",
    "        durations_t = torch.tensor(self.episode_durations, dtype=torch.float)\n",
    "        plt.title('Result')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Duration')\n",
    "        plt.plot(durations_t.numpy(),marker='o', label='Duraciones')\n",
    "        means = np.mean(durations_t.numpy())\n",
    "        plt.axhline(y=means, color='r', linestyle='--', label='Promedio')\n",
    "\n",
    "    def entrenar(self):\n",
    "        if torch.cuda.is_available():\n",
    "            self.num_episodes = 600\n",
    "        else:\n",
    "            self.num_episodes = 200\n",
    "        \n",
    "        loss = 0.0\n",
    "        for i_episode in range(self.num_episodes):\n",
    "            print(\"Episodio: {}\".format(i_episode))\n",
    "            # Initialize the environment and get it's state\n",
    "            state, info = self.env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "            duracion = 0.0\n",
    "            sum_rewards = 0.0\n",
    "            record = []\n",
    "            self.loss = 0.0\n",
    "            self.exploracion = 0.0\n",
    "            self.explotacion = 0.0\n",
    "            self.paso_episodio = 0\n",
    "            for t in count():\n",
    "                self.env.render()\n",
    "                \n",
    "                self.video_recorder.capture_frame()\n",
    "                \n",
    "                action = self.select_action(state)\n",
    "                observation, reward, terminated, truncated, _ = self.env.step(action.item())\n",
    "                reward = torch.tensor([reward], device=self.device)\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                sum_rewards += reward.item()\n",
    "                \n",
    "                if terminated:\n",
    "                    next_state = None\n",
    "                else:\n",
    "                    next_state = torch.tensor(observation, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "\n",
    "                # Store the transition in memory\n",
    "                self.memory.push(state, action, next_state, reward,duracion)\n",
    "                #duracion += torch.tensor(self.episode_durations, dtype=torch.float).numpy()\n",
    "                #self.memory.save(t)\n",
    "\n",
    "                # Move to the next state\n",
    "                state = next_state\n",
    "\n",
    "                # Perform one step of the optimization (on the policy network)\n",
    "                self.optimize_model()\n",
    "\n",
    "                # Soft update of the target network's weights\n",
    "                # θ′ ← τ θ + (1 −τ )θ′\n",
    "                target_net_state_dict = self.target_net.state_dict()\n",
    "                policy_net_state_dict = self.policy_net.state_dict()\n",
    "                for key in policy_net_state_dict:\n",
    "                    target_net_state_dict[key] = policy_net_state_dict[key]*self.TAU + target_net_state_dict[key]*(1-self.TAU)\n",
    "                self.target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "                if done:\n",
    "                    self.episode_durations.append(t + 1) \n",
    "                    duracion += 1                    \n",
    "                    break\n",
    "                    \n",
    "            record.append([i_episode,sum_rewards,self.loss,self.entropy_pol,self.paso_episodio,self.exploracion,(self.exploracion/sum_rewards),self.explotacion,(self.explotacion/sum_rewards)])\n",
    "            self.metricas_eval.append(record)\n",
    "        \n",
    "        print('Complete')\n",
    "        self.video_recorder.close()\n",
    "        self.video_recorder.enabled = False\n",
    "        self.save('./ckpt.pth')\n",
    "        self.env.close()   \n",
    "        \n",
    "        \n",
    "        \n",
    "        metricas = np.array(self.metricas_eval)\n",
    "        flattened_data = metricas.reshape(-1, 9).tolist()\n",
    "        df_metricas = pd.DataFrame(flattened_data,columns=['Episode','sum_rewwards','loss','entropy','paso_episodio','c_exploracion','p_exploracion','c_explotacion','p_explotacion'])\n",
    "        duracion = torch.tensor(self.episode_durations, dtype=torch.float).numpy()\n",
    "       \n",
    "        df_metricas['duration'] = duracion.tolist()\n",
    "        df_metricas.to_csv('./metricas.csv',index=False)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    def visualize_architecture(self):\n",
    "        model = self.policy_net  # Reemplaza \"YourModel\" con tu propia definición de modelo\n",
    "        x = torch.randn(1, 4)  # Crea una entrada de ejemplo para el modelo\n",
    "\n",
    "        # Genera el gráfico del modelo\n",
    "        y = model(x)\n",
    "        model_graph = draw_graph(\n",
    "        model, input_size=(4),\n",
    "        graph_name='RecursiveNet',\n",
    "        roll=True)\n",
    "        model_graph.visual_graph\n",
    "        #dot = make_dot(y, params=dict(model.named_parameters()),show_attrs=True, show_saved=True)\n",
    "        #print(dot)\n",
    "        # Guarda el gráfico como una imagen o muestra en pantalla\n",
    "        #dot.render(\"model_graph\", format=\"png\")  # Guarda \n",
    "        #Image(\"model_graph.png\")\n",
    "        #torch.onnx.export(model, y, \"model.onnx\", input_names=[\"Transition\"], output_names=[\"Action\"])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Main():\n",
    "    env = gym.make(\"CartPole-v1\",render_mode='rgb_array')    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    agente = Agente(device,env)\n",
    "    agente.visualize_architecture()\n",
    "    #agente.entrenar()\n",
    "    #agente.graficar_aprendizaje()\n",
    "    #agente.plot_durations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Main()\n",
      "Cell \u001b[1;32mIn[21], line 5\u001b[0m, in \u001b[0;36mMain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m agente \u001b[38;5;241m=\u001b[39m Agente(device,env)\n\u001b[1;32m----> 5\u001b[0m agente\u001b[38;5;241m.\u001b[39mvisualize_architecture()\n",
      "Cell \u001b[1;32mIn[20], line 265\u001b[0m, in \u001b[0;36mAgente.visualize_architecture\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# Genera el gráfico del modelo\u001b[39;00m\n\u001b[0;32m    264\u001b[0m y \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m--> 265\u001b[0m model_graph \u001b[38;5;241m=\u001b[39m draw_graph(\n\u001b[0;32m    266\u001b[0m model, input_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m4\u001b[39m),\n\u001b[0;32m    267\u001b[0m graph_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecursiveNet\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    268\u001b[0m roll\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    269\u001b[0m model_graph\u001b[38;5;241m.\u001b[39mvisual_graph\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchview\\torchview.py:211\u001b[0m, in \u001b[0;36mdraw_graph\u001b[1;34m(model, input_data, input_size, graph_name, depth, device, dtypes, mode, strict, expand_nested, graph_dir, hide_module_functions, hide_inner_tensors, roll, show_shapes, save_graph, filename, directory, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m edge_attr \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfontsize\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    204\u001b[0m }\n\u001b[0;32m    205\u001b[0m visual_graph \u001b[38;5;241m=\u001b[39m graphviz\u001b[38;5;241m.\u001b[39mDigraph(\n\u001b[0;32m    206\u001b[0m     name\u001b[38;5;241m=\u001b[39mgraph_name, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdot\u001b[39m\u001b[38;5;124m'\u001b[39m, strict\u001b[38;5;241m=\u001b[39mstrict,\n\u001b[0;32m    207\u001b[0m     graph_attr\u001b[38;5;241m=\u001b[39mgraph_attr, node_attr\u001b[38;5;241m=\u001b[39mnode_attr, edge_attr\u001b[38;5;241m=\u001b[39medge_attr,\n\u001b[0;32m    208\u001b[0m     directory\u001b[38;5;241m=\u001b[39mdirectory, filename\u001b[38;5;241m=\u001b[39mfilename\n\u001b[0;32m    209\u001b[0m )\n\u001b[1;32m--> 211\u001b[0m input_recorder_tensor, kwargs_record_tensor, input_nodes \u001b[38;5;241m=\u001b[39m process_input(\n\u001b[0;32m    212\u001b[0m     input_data, input_size, kwargs, device, dtypes\n\u001b[0;32m    213\u001b[0m )\n\u001b[0;32m    215\u001b[0m model_graph \u001b[38;5;241m=\u001b[39m ComputationGraph(\n\u001b[0;32m    216\u001b[0m     visual_graph, input_nodes, show_shapes, expand_nested,\n\u001b[0;32m    217\u001b[0m     hide_inner_tensors, hide_module_functions, roll, depth\n\u001b[0;32m    218\u001b[0m )\n\u001b[0;32m    220\u001b[0m forward_prop(\n\u001b[0;32m    221\u001b[0m     model, input_recorder_tensor, device, model_graph,\n\u001b[0;32m    222\u001b[0m     model_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_record_tensor\n\u001b[0;32m    223\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchview\\torchview.py:290\u001b[0m, in \u001b[0;36mprocess_input\u001b[1;34m(input_data, input_size, kwargs, device, dtypes)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 290\u001b[0m         dtypes \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mfloat] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_size)\n\u001b[0;32m    291\u001b[0m     correct_input_size \u001b[38;5;241m=\u001b[39m get_correct_input_sizes(input_size)\n\u001b[0;32m    292\u001b[0m     x \u001b[38;5;241m=\u001b[39m get_input_tensor(correct_input_size, dtypes, device)\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
